{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# PopGPT on Paperspace\nJust click \"RUN\" to run your new PopTorch code on the IPU!\n(You may need to log in first. Also add any imports your code needs.)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\nimport random\nfrom collections import deque, namedtuple\nfrom itertools import count\n\nimport gymnasium as gym\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport poptorch\n\nenv = gym.make(\"CartPole-v1\")\n\nis_ipython = \"inline\" in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\ndevice = torch.device(\"cpu\")\n\nTransition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n\n\nclass ReplayMemory(object):\n    def __init__(self, capacity):\n        self.memory = deque([], maxlen=capacity)\n\n    def push(self, *args):\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass DQN(nn.Module):\n    def __init__(self, n_observations, n_actions):\n        super(DQN, self).__init__()\n        self.layer1 = nn.Linear(n_observations, 128)\n        self.layer2 = nn.Linear(128, 128)\n        self.layer3 = nn.Linear(128, n_actions)\n\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        return self.layer3(x)\n\n\nBATCH_SIZE = 128\nGAMMA = 0.99\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 1000\nTAU = 0.005\nLR = 1e-4\n\nn_actions = env.action_space.n\nstate, info = env.reset()\nn_observations = len(state)\n\npolicy_net = DQN(n_observations, n_actions).to(device)\ntarget_net = DQN(n_observations, n_actions).to(device)\ntarget_net.load_state_dict(policy_net.state_dict())\n\noptimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\nmemory = ReplayMemory(10000)\n\nsteps_done = 0\n\n\ndef select_action(state):\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(\n        -1.0 * steps_done / EPS_DECAY\n    )\n    steps_done += 1\n    if sample > eps_threshold:\n        with torch.no_grad():\n            return policy_net(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor(\n            [[env.action_space.sample()]], device=device, dtype=torch.long\n        )\n\n\nepisode_durations = []\n\n\ndef plot_durations(show_result=False):\n    plt.figure(1)\n    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n    if show_result:\n        plt.title(\"Result\")\n    else:\n        plt.clf()\n        plt.title(\"Training...\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Duration\")\n    plt.plot(durations_t.numpy())\n    if len(durations_t) >= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)\n    if is_ipython:\n        if not show_result:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        else:\n            display.display(plt.gcf())\n\n\ndef optimize_model():\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    batch = Transition(*zip(*transitions))\n\n    non_final_mask = torch.tensor(\n        tuple(map(lambda s: s is not None, batch.next_state)),\n        device=device,\n        dtype=torch.bool,\n    )\n    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    with torch.no_grad():\n        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    criterion = nn.SmoothL1Loss()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n    optimizer.step()\n\n\nopts = poptorch.Options()\nopts.deviceIterations(1)\ntraining_data = poptorch.DataLoader(opts, dataset=memory, batch_size=BATCH_SIZE, shuffle=True)\n\npolicy_net = poptorch.trainingModel(policy_net, options=opts, optimizer=optimizer)\n\nfor i_episode in range(600):\n    state, info = env.reset()\n    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n    for t in count():\n        action = select_action(state)\n        observation, reward, terminated, truncated, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n        done = terminated or truncated\n\n        if terminated:\n            next_state = None\n        else:\n            next_state = torch.tensor(\n                observation, dtype=torch.float32, device=device\n            ).unsqueeze(0)\n\n        memory.push(state, action, next_state, reward)\n        state = next_state\n        optimize_model()\n        target_net_state_dict = target_net.state_dict()\n        policy_net_state_dict = policy_net.state_dict()\n        for key in policy_net_state_dict:\n            target_net_state_dict[key] = policy_net_state_dict[\n                key\n            ] * TAU + target_net_state_dict[key] * (1 - TAU)\n        target_net.load_state_dict(target_net_state_dict)\n\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n\nprint(\"Complete\")\nplot_durations(show_result=True)\nplt.ioff()\nplt.show()\n"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
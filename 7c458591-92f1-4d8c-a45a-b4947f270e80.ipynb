{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# PopGPT on Paperspace\nJust click \"RUN\" to run your new PopTorch code on the IPU!\n(You may need to log in first. Also add any imports your code needs.)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\nimport os\nfrom tempfile import TemporaryDirectory\nfrom typing import Tuple\n\nimport torch\nimport poptorch\nfrom torch import Tensor, nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import dataset\n\n\nclass TransformerModel(nn.Module):\n    def __init__(\n        self,\n        ntoken: int,\n        d_model: int,\n        nhead: int,\n        d_hid: int,\n        nlayers: int,\n        dropout: float = 0.5,\n    ):\n        super().__init__()\n        self.model_type = \"Transformer\"\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\n\ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n        )\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = x + self.pe[: x.size(0)]\n        return self.dropout(x)\n\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.datasets import WikiText2\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntrain_iter = WikiText2(split=\"train\")\ntokenizer = get_tokenizer(\"basic_english\")\nvocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n\ndef data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [\n        torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter\n    ]\n    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n\n\ntrain_iter, val_iter, test_iter = WikiText2()\ntrain_data = data_process(train_iter)\nval_data = data_process(val_iter)\ntest_data = data_process(test_iter)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef batchify(data: Tensor, bsz: int) -> Tensor:\n    seq_len = data.size(0) // bsz\n    data = data[: seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to(device)\n\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\nval_data = batchify(val_data, eval_batch_size)\ntest_data = batchify(test_data, eval_batch_size)\n\n\nbptt = 35\n\n\ndef get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i : i + seq_len]\n    target = source[i + 1 : i + 1 + seq_len].reshape(-1)\n    return data, target\n\n\nntokens = len(vocab)\nemsize = 200\nd_hid = 200\nnlayers = 2\nnhead = 2\ndropout = 0.2\nmodel = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n\nimport time\n\ncriterion = nn.CrossEntropyLoss()\nlr = 5.0\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n# PopTorch options\nopts = poptorch.Options()\nopts.deviceIterations(bptt)\npoptorch_model = poptorch.trainingModel(model, options=opts, optimizer=optimizer)\n\ndef train(model: nn.Module) -> None:\n    model.train()\n    total_loss = 0.0\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(bptt).to(device)\n\n    num_batches = len(train_data) // bptt\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        seq_len = data.size(0)\n        if seq_len != bptt:  # only on last batch\n            src_mask = src_mask[:seq_len, :seq_len]\n        output = model(data, src_mask)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        # Update the model in one step\n        _, _ = poptorch_model(data, src_mask, targets)\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(\n                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n            )\n            total_loss = 0\n            start_time = time.time()\n\n\ndef evaluate(model: nn.Module, eval_data: Tensor) -> float:\n    model.eval()\n    total_loss = 0.0\n    src_mask = generate_square_subsequent_mask(bptt).to(device)\n    with torch.no_grad():\n        for i in range(0, eval_data.size(0) - 1, bptt):\n            data, targets = get_batch(eval_data, i)\n            seq_len = data.size(0)\n            if seq_len != bptt:\n                src_mask = src_mask[:seq_len, :seq_len]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += seq_len * criterion(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)\n\n\nbest_val_loss = float(\"inf\")\nepochs = 3\n\nwith TemporaryDirectory() as tempdir:\n    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, val_data)\n        val_ppl = math.exp(val_loss)\n        elapsed = time.time() - epoch_start_time\n        print(\"-\" * 89)\n        print(\n            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n        )\n        print(\"-\" * 89)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_params_path)\n\n    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n\ntest_loss = evaluate(model, test_data)\ntest_ppl = math.exp(test_loss)\nprint(\"=\" * 89)\nprint(f\"| End of training | test loss {test_loss:5.2f} | test ppl {test_ppl:8.2f}\")\nprint(\"=\" * 89)\n"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}